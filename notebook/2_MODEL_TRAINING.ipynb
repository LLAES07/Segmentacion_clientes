{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas relevantes para el clustering jerárquico\n",
    "X = df_filtrado[['Recencia', 'Monto', 'Frecuencia']]\n",
    "\n",
    "# Crear lista de métodos de enlace\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "# Configurar la visualización\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Iterar sobre los métodos de enlace y realizar clustering jerárquico\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    # Realizar clustering jerárquico con el método de enlace actual\n",
    "    clustering = AgglomerativeClustering(n_clusters=3, linkage=method)\n",
    "    labels = clustering.fit_predict(X)\n",
    "    sc = silhouette_score(X, labels)\n",
    "\n",
    "    # Graficar los puntos en un diagrama de dispersión para las dos primeras columnas\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.scatter(X['Recencia'], X['Monto'], c=labels, cmap='viridis')\n",
    "    plt.xlabel('Recencia')\n",
    "    plt.ylabel('Monto')\n",
    "    plt.title(f'Clustering jerárquico - Enlace: {method} - s-score: {sc:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer\n",
    "\n",
    "# Estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_filtrado[['Recencia', 'Frecuencia', 'Monto']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definir una lista de posibles valores de k para el método del codo\n",
    "k_values = range(2, 8)\n",
    "\n",
    "# Inicializar listas para almacenar las métricas\n",
    "inertia_values = []\n",
    "\n",
    "# Realizar clustering con diferentes valores de k y calcular las métricas\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    labels = kmeans.labels_\n",
    "    # Calcular la inercia\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Graficar el método del codo utilizando la inercia\n",
    "plt.plot(k_values, inertia_values, 'bo-')\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('Inercia')\n",
    "plt.title('Método del Codo')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar algoritmos de clustering (por ejemplo, KMeans)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df_filtrado['Cluster'] = kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# Visualizar los resultados\n",
    "plt.scatter(df_filtrado['Recencia'], df_filtrado['Monto'], c=df_filtrado['Cluster'], cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Recencia')\n",
    "plt.ylabel('Monto')\n",
    "plt.title('Clusters generados por KMeans')\n",
    "\n",
    "# Obtener etiquetas únicas de los clusters\n",
    "unique_labels = df_filtrado['Cluster'].unique()\n",
    "\n",
    "# Generar etiquetas para la leyenda\n",
    "legend_labels = [f'Cluster {label}' for label in unique_labels]\n",
    "print(legend_labels)\n",
    "# Agregar leyenda\n",
    "plt.legend(legend_labels, loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=2)  # Crear el modelo PCA\n",
    "df_pca = pca.fit_transform(df_scaled)  # Transformar los datos\n",
    "\n",
    "# Opcional: Para acceder a la varianza explicada\n",
    "explained_variance = pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(df_pca, columns=[\"PC1\", \"PC2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Crear un scatter plot de las dos primeras componentes principales\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df_pca.iloc[:, 0], y=df_pca.iloc[:, 1], hue=df_filtrado['Cluster'], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.title(\"Visualización de PCA en 2D\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = silhouette_score(df_pca, df_filtrado['Cluster'])\n",
    "print(\"Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear el modelo DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Ajusta eps y min_samples según sea necesario\n",
    "df_filtrado[\"Cluster_DBSCAN\"] = dbscan.fit_predict(df_pca)\n",
    "\n",
    "# Visualizar los resultados\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df_pca.iloc[:, 0], y=df_pca.iloc[:, 1], hue=df_filtrado[\"Cluster_DBSCAN\"], palette=\"tab10\", legend=\"full\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.title(\"DBSCAN aplicado a los datos reducidos con PCA\")\n",
    "plt.legend(title=\"Cluster (DBSCAN)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = silhouette_score(df_pca, df_filtrado['Cluster_DBSCAN'])\n",
    "print(\"Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definir rangos para eps y min_samples\n",
    "eps_range = np.arange(0.1, 1.0, 0.1)  # Valores de eps\n",
    "min_samples_range = range(3, 11)      # Valores de min_samples\n",
    "\n",
    "# Variables para almacenar los mejores parámetros\n",
    "best_score = -1\n",
    "best_params = {\"eps\": None, \"min_samples\": None}\n",
    "\n",
    "# GridSearch manual\n",
    "for eps in eps_range:\n",
    "    for min_samples in min_samples_range:\n",
    "        # Aplicar DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(df_pca)\n",
    "        \n",
    "        # Ignorar configuraciones con un único clúster o todos ruido\n",
    "        if len(set(labels)) <= 1 or (labels == -1).mean() == 1.0:\n",
    "            continue\n",
    "        \n",
    "        # Calcular Silhouette Score\n",
    "        score = silhouette_score(df_pca, labels)\n",
    "        \n",
    "        # Guardar si es el mejor hasta ahora\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {\"eps\": eps, \"min_samples\": min_samples}\n",
    "\n",
    "# Resultados\n",
    "print(\"Mejor Silhouette Score:\", best_score)x\n",
    "print(\"Mejores parámetros:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtrado['CLUSTER_BEST_INT'] = df_filtrado['Cluster_DBSCAN_BEST'].astype(int)\n",
    "# Identificar columnas categóricas\n",
    "categorical_cols = df_filtrado.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "# Excluir columnas categóricas antes de calcular el promedio\n",
    "numerical_cols = df_filtrado.drop(columns=categorical_cols).columns\n",
    "\n",
    "# Agrupación por \"CLUSTER_BEST_INT\" y cálculo de la media\n",
    "cluster_summary = df_filtrado[numerical_cols].groupby(df_filtrado[\"CLUSTER_BEST_INT\"]).mean()\n",
    "cluster_summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
